
lapFusionMPI9_hybrid.c


ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ module load gcc/6.1.0 openmpi/1.8.1
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpicc -g -Wall -lm -Ofast -fopenmp -o lapMPI9_HY lapFusionMPI9_hybrid.c
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 1000
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 0.687867

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 1000
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 0.517982

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 2000
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.017999, A[15][15]= 0.012064
 [0]            time: 4.576035

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 2000
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.017999, A[15][15]= 0.012064
 [0]            time: 4.105426

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 3000
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.022895, A[23][23]= 0.007810
 [0]            time: 9.281551

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 3000
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.022895, A[23][23]= 0.007810
 [0]            time: 9.694333

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 4000
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.024827, A[31][31]= 0.004645
 [0]            time: 16.144432

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 4000
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.024827, A[31][31]= 0.004645
 [0]            time: 16.213485

HYBRID With TAU....

HY_Core 2

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ tau_cc.sh -o lapMPI9_HY -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.c 
Debug: Using compiler-based instrumentation


Debug: Compiling (Individually) with Instrumented Code
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp -I. -c lapFusionMPI9_hybrid.c -g -DPROFILING_ON -DTAU_GNU -DTAU_DOT_H_LESS_HEADERS -DTAU_MPI -DTAU_UNIFY -DTAU_MPI_THREADED -DTAU_LINUX_TIMERS -DTAU_MPIGREQUEST -DTAU_MPIDATAREP -DTAU_MPIERRHANDLER -DTAU_MPICONSTCHAR -DTAU_MPIATTRFUNCTION -DTAU_MPITYPEEX -DTAU_MPIADDERROR -DTAU_LARGEFILE -D_LARGEFILE64_SOURCE -DTAU_MPIFILE -DHAVE_TR1_HASH_MAP -DTAU_SS_ALLOC_SUPPORT -DEBS_CLOCK_RES=1 -DTAU_STRSIGNAL_OK -DTAU_TRACK_LD_LOADER -DTAU_MPICH3 -DTAU_MPI_EXTENSIONS -I/home/master/ppM/ppM-1-1/TAU/tau_install//include -I/soft/openmpi-1.8.1/include -I/soft/openmpi-1.8.1/include/openmpi -I/soft/openmpi-1.8.1/include/openmpi/ompi -I/soft/openmpi-1.8.1/lib -o lapFusionMPI9_hybrid.o -g -finstrument-functions -finstrument-functions-exclude-file-list=/usr/include


Debug: Linking (Together) object files
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.o -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -lTauMpi-mpi -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -ltau-mpi -Wl,--export-dynamic -lrt -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -ldl -lm -L/soft/gcc-6.1.0/lib/gcc/x86_64-pc-linux-gnu/6.1.0/ -lstdc++ -lgcc_s -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib/static-mpi -g -o lapMPI9_HY

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ mpirun -np 2 lapMPI9_HY 1000
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 34.565681


ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ pprof
Reading Profile files in profile.*

NODE 0;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.042       34,935           1           1   34935470 .TAU application
100.0           13       34,935           1        4010   34935428 addr=<0x4330e0>
 98.3       34,319       34,348        1000      200002      34349 addr=<0x433e70>
  0.8          287          287           1           0     287927 MPI_Init_thread()
  0.5          191          191        1000           0        191 MPI_Reduce()
  0.2           81           81           1           0      81732 MPI_Finalize()
  0.0           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            3            3        1000           0          3 MPI_Recv()
  0.0            2            2           2           0       1253 addr=<0x433cf0>
  0.0            2            2        1000           0          2 MPI_Send()
  0.0            1            1           1           0       1958 MPI_Scatter()
  0.0            1            1           1           0       1067 MPI_Gather()
  0.0        0.905        0.905           1           0        905 addr=<0x433b20>
  0.0        0.039        0.039           1           0         39 MPI_Bcast()
  0.0            0            0           1           0          0 MPI_Comm_rank()
  0.0            0            0           1           0          0 MPI_Comm_size()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 0, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         1      2E+06      2E+06      2E+06          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      2E+06      2E+06      2E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

NODE 1;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.038       34,934           1           1   34934526 .TAU application
100.0           15       34,934           1        4010   34934488 addr=<0x4330e0>
 98.0       34,205       34,235        1000      200002      34235 addr=<0x433e70>
  0.9          299          299        1000           0        300 MPI_Recv()
  0.8          287          287           1           0     287017 MPI_Init_thread()
  0.2           81           81           1           0      81776 MPI_Finalize()
  0.0           15           15      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            5            5        1000           0          6 MPI_Reduce()
  0.0            2            2        1000           0          3 MPI_Send()
  0.0            2            2           2           0       1260 addr=<0x433cf0>
  0.0            1            1           1           0       1768 MPI_Gather()
  0.0            1            1           1           0       1711 MPI_Scatter()
  0.0        0.828        0.828           1           0        828 addr=<0x433b20>
  0.0        0.045        0.045           1           0         45 MPI_Bcast()
  0.0        0.001        0.001           1           0          1 MPI_Comm_size()
  0.0            0            0           1           0          0 MPI_Comm_rank()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 1, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         0          0          0          0          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      2E+06      2E+06      2E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

FUNCTION SUMMARY (total):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0         0.08     1:09.869           2           2   34934998 .TAU application
100.0           29     1:09.869           2        8020   34934958 addr=<0x4330e0>
 98.2     1:08.524     1:08.584        2000      400004      34292 addr=<0x433e70>
  0.8          574          574           2           0     287472 MPI_Init_thread()
  0.4          302          302        2000           0        151 MPI_Recv()
  0.3          196          196        2000           0         98 MPI_Reduce()
  0.2          163          163           2           0      81754 MPI_Finalize()
  0.0           30           30      200002           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           29           29      200002           0          0 addr=<0x433a50> [THROTTLED]
  0.0            5            5           4           0       1257 addr=<0x433cf0>
  0.0            4            4        2000           0          2 MPI_Send()
  0.0            3            3           2           0       1834 MPI_Scatter()
  0.0            2            2           2           0       1418 MPI_Gather()
  0.0            1            1           2           0        866 addr=<0x433b20>
  0.0        0.084        0.084           2           0         42 MPI_Bcast()
  0.0        0.001        0.001           2           0          0 MPI_Comm_size()
  0.0            0            0           2           0          0 MPI_Comm_rank()

FUNCTION SUMMARY (mean):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0         0.04       34,934           1           1   34934998 .TAU application
100.0           14       34,934           1        4010   34934958 addr=<0x4330e0>
 98.2       34,262       34,292        1000      200002      34292 addr=<0x433e70>
  0.8          287          287           1           0     287472 MPI_Init_thread()
  0.4          151          151        1000           0        151 MPI_Recv()
  0.3           98           98        1000           0         98 MPI_Reduce()
  0.2           81           81           1           0      81754 MPI_Finalize()
  0.0           15           15      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            2            2           2           0       1257 addr=<0x433cf0>
  0.0            2            2        1000           0          2 MPI_Send()
  0.0            1            1           1           0       1834 MPI_Scatter()
  0.0            1            1           1           0       1418 MPI_Gather()
  0.0        0.867        0.867           1           0        866 addr=<0x433b20>
  0.0        0.042        0.042           1           0         42 MPI_Bcast()
  0.0       0.0005       0.0005           1           0          0 MPI_Comm_size()
  0.0            0            0           1           0          0 MPI_Comm_rank()

ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ module load gcc/6.1.0 openmpi/1.8.1
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpicc -g -Wall -lm -Ofast -fopenmp -o lapMPI9_HY lapFusionMPI9_hybrid.c
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 1000
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 0.702743
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 2000
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.017999, A[15][15]= 0.012064
 [0]            time: 4.263784
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 3000
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.022895, A[23][23]= 0.007810
 [0]            time: 9.339001
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 4000
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.024827, A[31][31]= 0.004645
 [0]            time: 16.116379
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 4000
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.024827, A[31][31]= 0.004645
 [0]            time: 16.450410
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 3000
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.022895, A[23][23]= 0.007810
 [0]            time: 9.462874
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ clear
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ module load gcc/6.1.0 openmpi/1.8.1
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpicc -g -Wall -lm -Ofast -fopenmp -o lapMPI9_HY lapFusionMPI9_hybrid.c
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 1000
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 0.687867
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 1000
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 0.517982
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 2000
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.017999, A[15][15]= 0.012064
 [0]            time: 4.576035
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 2000
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 2000 x 2000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.017999, A[15][15]= 0.012064
 [0]            time: 4.105426
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 3000
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.022895, A[23][23]= 0.007810
 [0]            time: 9.281551
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 3000
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 3000 x 3000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.022895, A[23][23]= 0.007810
 [0]            time: 9.694333
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 2 lapMPI9_HY 4000
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.024827, A[31][31]= 0.004645
 [0]            time: 16.144432
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ mpirun -np 4 lapMPI9_HY 4000
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 4000 x 4000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.024827, A[31][31]= 0.004645
 [0]            time: 16.213485
ppM-1-1@aolin21:~/Escritorio/MPI/BasicTAU/hybrid$ cd
ppM-1-1@aolin21:~$ cd Escritorio/MPI/Basic&
Display all 3575 possibilities? (y or n)
ppM-1-1@aolin21:~$ cd Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2/
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export PATH=/home/master/ppM/ppM-1-1/TAU/tau_install/x86_64/bin:$           PATH
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export TAU_MAKEFILE=/home/master/ppM/ppM-1-1/TAU/tau_install/x86_           64/lib/Makefile.tau-mpi
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export TAU_OPTIONS=-optCompInst
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ tau_cc.sh -o lapMPI_HY9 -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.c Debug: Using compiler-based instrumentation


Debug: Compiling (Individually) with Instrumented Code
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp -I. -c lapFusionMPI9_hybrid.c -g -DPROFILING_ON -DTAU_GNU -DTAU_DOT_H_LESS_HEADERS -DTAU_MPI -DTAU_UNIFY -DTAU_MPI_THREADED -DTAU_LINUX_TIMERS -DTAU_MPIGREQUEST -DTAU_MPIDATAREP -DTAU_MPIERRHANDLER -DTAU_MPICONSTCHAR -DTAU_MPIATTRFUNCTION -DTAU_MPITYPEEX -DTAU_MPIADDERROR -DTAU_LARGEFILE -D_LARGEFILE64_SOURCE -DTAU_MPIFILE -DHAVE_TR1_HASH_MAP -DTAU_SS_ALLOC_SUPPORT -DEBS_CLOCK_RES=1 -DTAU_STRSIGNAL_OK -DTAU_TRACK_LD_LOADER -DTAU_MPICH3 -DTAU_MPI_EXTENSIONS -I/home/master/ppM/ppM-1-1/TAU/tau_install//include -I/soft/openmpi-1.8.1/include -I/soft/openmpi-1.8.1/include/openmpi -I/soft/openmpi-1.8.1/include/openmpi/ompi -I/soft/openmpi-1.8.1/lib -o lapFusionMPI9_hybrid.o -g -finstrument-functions -finstrument-functions-exclude-file-list=/usr/include


Debug: Linking (Together) object files
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.o -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -lTauMpi-mpi -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -ltau-mpi -Wl,--export-dynamic -lrt -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -ldl -lm -L/soft/gcc-6.1.0/lib/gcc/x86_64-pc-linux-gnu/6.1.0/ -lstdc++ -lgcc_s -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib/static-mpi -g -o lapMPI_HY9

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ mpirun -np 2 lapMPI9_HY 1000
--------------------------------------------------------------------------
mpirun was unable to find the specified executable file, and therefore
did not launch the job.  This error was first reported for process
rank 0; it may have occurred for other processes as well.

NOTE: A common cause for this error is misspelling a mpirun command
      line parameter option (remember that mpirun interprets the first
      unrecognized command line token as the executable).

Node:       aolin21
Executable: lapMPI9_HY
--------------------------------------------------------------------------
2 total processes failed to start
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ ls
lapFusionMPI9_hybrid.c  lapFusionMPI9_hybrid.o  lapMPI_HY9
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ tau_cc.sh -o lapMPI9_HY -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.c 
Debug: Using compiler-based instrumentation


Debug: Compiling (Individually) with Instrumented Code
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp -I. -c lapFusionMPI9_hybrid.c -g -DPROFILING_ON -DTAU_GNU -DTAU_DOT_H_LESS_HEADERS -DTAU_MPI -DTAU_UNIFY -DTAU_MPI_THREADED -DTAU_LINUX_TIMERS -DTAU_MPIGREQUEST -DTAU_MPIDATAREP -DTAU_MPIERRHANDLER -DTAU_MPICONSTCHAR -DTAU_MPIATTRFUNCTION -DTAU_MPITYPEEX -DTAU_MPIADDERROR -DTAU_LARGEFILE -D_LARGEFILE64_SOURCE -DTAU_MPIFILE -DHAVE_TR1_HASH_MAP -DTAU_SS_ALLOC_SUPPORT -DEBS_CLOCK_RES=1 -DTAU_STRSIGNAL_OK -DTAU_TRACK_LD_LOADER -DTAU_MPICH3 -DTAU_MPI_EXTENSIONS -I/home/master/ppM/ppM-1-1/TAU/tau_install//include -I/soft/openmpi-1.8.1/include -I/soft/openmpi-1.8.1/include/openmpi -I/soft/openmpi-1.8.1/include/openmpi/ompi -I/soft/openmpi-1.8.1/lib -o lapFusionMPI9_hybrid.o -g -finstrument-functions -finstrument-functions-exclude-file-list=/usr/include


Debug: Linking (Together) object files
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.o -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -lTauMpi-mpi -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -ltau-mpi -Wl,--export-dynamic -lrt -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -ldl -lm -L/soft/gcc-6.1.0/lib/gcc/x86_64-pc-linux-gnu/6.1.0/ -lstdc++ -lgcc_s -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib/static-mpi -g -o lapMPI9_HY

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ mpirun -np 2 lapMPI9_HY 1000
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 34.565681
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ pprof
Reading Profile files in profile.*

NODE 0;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.042       34,935           1           1   34935470 .TAU application
100.0           13       34,935           1        4010   34935428 addr=<0x4330e0>
 98.3       34,319       34,348        1000      200002      34349 addr=<0x433e70>
  0.8          287          287           1           0     287927 MPI_Init_thread()
  0.5          191          191        1000           0        191 MPI_Reduce()
  0.2           81           81           1           0      81732 MPI_Finalize()
  0.0           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            3            3        1000           0          3 MPI_Recv()
  0.0            2            2           2           0       1253 addr=<0x433cf0>
  0.0            2            2        1000           0          2 MPI_Send()
  0.0            1            1           1           0       1958 MPI_Scatter()
  0.0            1            1           1           0       1067 MPI_Gather()
  0.0        0.905        0.905           1           0        905 addr=<0x433b20>
  0.0        0.039        0.039           1           0         39 MPI_Bcast()
  0.0            0            0           1           0          0 MPI_Comm_rank()
  0.0            0            0           1           0          0 MPI_Comm_size()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 0, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         1      2E+06      2E+06      2E+06          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      2E+06      2E+06      2E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

NODE 1;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.038       34,934           1           1   34934526 .TAU application
100.0           15       34,934           1        4010   34934488 addr=<0x4330e0>
 98.0       34,205       34,235        1000      200002      34235 addr=<0x433e70>
  0.9          299          299        1000           0        300 MPI_Recv()
  0.8          287          287           1           0     287017 MPI_Init_thread()
  0.2           81           81           1           0      81776 MPI_Finalize()
  0.0           15           15      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            5            5        1000           0          6 MPI_Reduce()
  0.0            2            2        1000           0          3 MPI_Send()
  0.0            2            2           2           0       1260 addr=<0x433cf0>
  0.0            1            1           1           0       1768 MPI_Gather()
  0.0            1            1           1           0       1711 MPI_Scatter()
  0.0        0.828        0.828           1           0        828 addr=<0x433b20>
  0.0        0.045        0.045           1           0         45 MPI_Bcast()
  0.0        0.001        0.001           1           0          1 MPI_Comm_size()
  0.0            0            0           1           0          0 MPI_Comm_rank()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 1, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         0          0          0          0          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      2E+06      2E+06      2E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

FUNCTION SUMMARY (total):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0         0.08     1:09.869           2           2   34934998 .TAU application
100.0           29     1:09.869           2        8020   34934958 addr=<0x4330e0>
 98.2     1:08.524     1:08.584        2000      400004      34292 addr=<0x433e70>
  0.8          574          574           2           0     287472 MPI_Init_thread()
  0.4          302          302        2000           0        151 MPI_Recv()
  0.3          196          196        2000           0         98 MPI_Reduce()
  0.2          163          163           2           0      81754 MPI_Finalize()
  0.0           30           30      200002           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           29           29      200002           0          0 addr=<0x433a50> [THROTTLED]
  0.0            5            5           4           0       1257 addr=<0x433cf0>
  0.0            4            4        2000           0          2 MPI_Send()
  0.0            3            3           2           0       1834 MPI_Scatter()
  0.0            2            2           2           0       1418 MPI_Gather()
  0.0            1            1           2           0        866 addr=<0x433b20>
  0.0        0.084        0.084           2           0         42 MPI_Bcast()
  0.0        0.001        0.001           2           0          0 MPI_Comm_size()
  0.0            0            0           2           0          0 MPI_Comm_rank()

FUNCTION SUMMARY (mean):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0         0.04       34,934           1           1   34934998 .TAU application
100.0           14       34,934           1        4010   34934958 addr=<0x4330e0>
 98.2       34,262       34,292        1000      200002      34292 addr=<0x433e70>
  0.8          287          287           1           0     287472 MPI_Init_thread()
  0.4          151          151        1000           0        151 MPI_Recv()
  0.3           98           98        1000           0         98 MPI_Reduce()
  0.2           81           81           1           0      81754 MPI_Finalize()
  0.0           15           15      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.0           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            2            2           2           0       1257 addr=<0x433cf0>
  0.0            2            2        1000           0          2 MPI_Send()
  0.0            1            1           1           0       1834 MPI_Scatter()
  0.0            1            1           1           0       1418 MPI_Gather()
  0.0        0.867        0.867           1           0        866 addr=<0x433b20>
  0.0        0.042        0.042           1           0         42 MPI_Bcast()
  0.0       0.0005       0.0005           1           0          0 MPI_Comm_size()
  0.0            0            0           1           0          0 MPI_Comm_rank()
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ paraprof
Exception in thread "AWT-EventQueue-0" java.awt.HeadlessException:
No X11 DISPLAY variable was set, but this program performed an operation which requires it.
        at java.awt.GraphicsEnvironment.checkHeadless(GraphicsEnvironment.java:204)
        at java.awt.Window.<init>(Window.java:536)
        at java.awt.Frame.<init>(Frame.java:420)
        at java.awt.Frame.<init>(Frame.java:385)
        at javax.swing.JFrame.<init>(JFrame.java:189)
        at edu.uoregon.tau.paraprof.ParaProfErrorDialog.<init>(ParaProfErrorDialog.java:37)
        at edu.uoregon.tau.paraprof.ParaProfUtils.handleException(ParaProfUtils.java:1719)
        at edu.uoregon.tau.paraprof.ParaProf$1.run(ParaProf.java:754)
        at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311)
        at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:756)
        at java.awt.EventQueue.access$500(EventQueue.java:97)
        at java.awt.EventQueue$3.run(EventQueue.java:709)
        at java.awt.EventQueue$3.run(EventQueue.java:703)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:80)
        at java.awt.EventQueue.dispatchEvent(EventQueue.java:726)
        at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
        at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
        at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
        at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
        at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
        at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$



##### HY_Core 4.....

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export TAU_TRACE=0
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ module load gcc/6.1.0 openmpi/1.8.1                                         ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export TAU_TRACE=0
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export PATH=/home/master/ppM/ppM-1-1/TAU/tau_install/x86_64/bin:$PATH       ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export TAU_MAKEFILE=/home/master/ppM/ppM-1-1/TAU/tau_install/x86_64/lib/Makefile.tau-mpi
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ export TAU_OPTIONS=-optCompInst
ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ tau_cc.sh -o lapMPI9_HY -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.c
Debug: Using compiler-based instrumentation


Debug: Compiling (Individually) with Instrumented Code
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp -I. -c lapFusionMPI9_hybrid.c -g -DPROFILING_ON -DTAU_GNU -DTAU_DOT_H_LESS_HEADERS -DTAU_MPI -DTAU_UNIFY -DTAU_MPI_THREADED -DTAU_LINUX_TIMERS -DTAU_MPIGREQUEST -DTAU_MPIDATAREP -DTAU_MPIERRHANDLER -DTAU_MPICONSTCHAR -DTAU_MPIATTRFUNCTION -DTAU_MPITYPEEX -DTAU_MPIADDERROR -DTAU_LARGEFILE -D_LARGEFILE64_SOURCE -DTAU_MPIFILE -DHAVE_TR1_HASH_MAP -DTAU_SS_ALLOC_SUPPORT -DEBS_CLOCK_RES=1 -DTAU_STRSIGNAL_OK -DTAU_TRACK_LD_LOADER -DTAU_MPICH3 -DTAU_MPI_EXTENSIONS -I/home/master/ppM/ppM-1-1/TAU/tau_install//include -I/soft/openmpi-1.8.1/include -I/soft/openmpi-1.8.1/include/openmpi -I/soft/openmpi-1.8.1/include/openmpi/ompi -I/soft/openmpi-1.8.1/lib -o lapFusionMPI9_hybrid.o -g -finstrument-functions -finstrument-functions-exclude-file-list=/usr/include


Debug: Linking (Together) object files
Executing> /soft/gcc-6.1.0/bin/gcc -g -Wall -lm -Ofast -fopenmp lapFusionMPI9_hybrid.o -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -lTauMpi-mpi -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib -ltau-mpi -Wl,--export-dynamic -lrt -L/soft/openmpi-1.8.1/lib -lmpi -lmpi_cxx -Wl,-rpath,/soft/openmpi-1.8.1/lib -ldl -lm -L/soft/gcc-6.1.0/lib/gcc/x86_64-pc-linux-gnu/6.1.0/ -lstdc++ -lgcc_s -L/home/master/ppM/ppM-1-1/TAU/tau_install//x86_64/lib/static-mpi -g -o lapMPI9_HY

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ mpirun -np 4 lapMPI9_HY 1000
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
TAU Warning: BFD is not available in at least one part of this TAU-instrumented application! Please check to see if BFD is not shared or not present. Expect some missing BFD functionality.
TAU Warning: Comp_gnu - BFD is not available during TAU build. Symbols may not be resolved!
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Jacobi relaxation Calculation: 1000 x 1000 mesh, maximum of 1000 iterations
Total Iterations:  1000, ERROR: 0.015535, A[7][7]= 0.016615
 [0]            time: 17.574133

ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ pprof
Reading Profile files in profile.*

NODE 0;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.044       17,949           1           1   17949416 .TAU application
100.0           12       17,949           1        4010   17949372 addr=<0x4330e0>
 96.4       17,279       17,308        1000      200002      17309 addr=<0x433e70>
  1.6          292          292           1           0     292775 MPI_Init_thread()
  1.3          239          239        1000           0        239 MPI_Reduce()
  0.5           82           82           1           0      82382 MPI_Finalize()
  0.1           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.1           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            4            4           2           0       2428 addr=<0x433cf0>
  0.0            2            2        1000           0          3 MPI_Recv()
  0.0            2            2        1000           0          2 MPI_Send()
  0.0            2            2           1           0       2049 MPI_Scatter()
  0.0            1            1           1           0       1258 MPI_Gather()
  0.0        0.484        0.484           1           0        484 addr=<0x433b20>
  0.0        0.034        0.034           1           0         34 MPI_Bcast()
  0.0            0            0           1           0          0 MPI_Comm_rank()
  0.0            0            0           1           0          0 MPI_Comm_size()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 0, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         1      1E+06      1E+06      1E+06          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      1E+06      1E+06      1E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

NODE 1;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.039       17,948           1           1   17948946 .TAU application
100.0           14       17,948           1        6010   17948907 addr=<0x4330e0>
 96.7       17,326       17,356        1000      200002      17356 addr=<0x433e70>
  1.6          292          292           1           0     292392 MPI_Init_thread()
  1.0          186          186        2000           0         93 MPI_Recv()
  0.5           82           82           1           0      82664 MPI_Finalize()
  0.1           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.1           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            4            4        1000           0          5 MPI_Reduce()
  0.0            4            4           2           0       2410 addr=<0x433cf0>
  0.0            3            3        2000           0          2 MPI_Send()
  0.0            1            1           1           0       1719 MPI_Gather()
  0.0            1            1           1           0       1034 MPI_Scatter()
  0.0        0.433        0.433           1           0        433 addr=<0x433b20>
  0.0        0.038        0.038           1           0         38 MPI_Bcast()
  0.0        0.001        0.001           1           0          1 MPI_Comm_size()
  0.0            0            0           1           0          0 MPI_Comm_rank()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 1, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         0          0          0          0          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      1E+06      1E+06      1E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

NODE 2;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.053       17,946           1           1   17946653 .TAU application
100.0           15       17,946           1        6010   17946600 addr=<0x4330e0>
 96.4       17,263       17,293        1000      200002      17294 addr=<0x433e70>
  1.6          289          289           1           0     289999 MPI_Init_thread()
  1.4          248          248        2000           0        124 MPI_Recv()
  0.5           82           82           1           0      82541 MPI_Finalize()
  0.1           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.1           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            4            4           2           0       2388 addr=<0x433cf0>
  0.0            4            4        1000           0          4 MPI_Reduce()
  0.0            3            3        2000           0          2 MPI_Send()
  0.0            1            1           1           0       1897 MPI_Gather()
  0.0            1            1           1           0       1549 MPI_Scatter()
  0.0        0.444        0.444           1           0        444 addr=<0x433b20>
  0.0        0.042        0.042           1           0         42 MPI_Bcast()
  0.0            0            0           1           0          0 MPI_Comm_rank()
  0.0            0            0           1           0          0 MPI_Comm_size()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 2, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         0          0          0          0          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      1E+06      1E+06      1E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

NODE 3;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0         0.04       17,945           1           1   17945358 .TAU application
100.0           13       17,945           1        4010   17945318 addr=<0x4330e0>
 96.6       17,301       17,330        1000      200002      17331 addr=<0x433e70>
  1.6          288          288           1           0     288742 MPI_Init_thread()
  1.2          213          213        1000           0        214 MPI_Recv()
  0.5           82           82           1           0      82406 MPI_Finalize()
  0.1           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.1           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            4            4           2           0       2462 addr=<0x433cf0>
  0.0            4            4        1000           0          4 MPI_Reduce()
  0.0            2            2        1000           0          2 MPI_Send()
  0.0            1            1           1           0       1971 MPI_Gather()
  0.0            1            1           1           0       1922 MPI_Scatter()
  0.0        0.412        0.412           1           0        412 addr=<0x433b20>
  0.0        0.101        0.101           1           0        101 MPI_Bcast()
  0.0            0            0           1           0          0 MPI_Comm_rank()
  0.0            0            0           1           0          0 MPI_Comm_size()
---------------------------------------------------------------------------------------

USER EVENTS Profile :NODE 3, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
         1          4          4          4          0  Message size for broadcast
         0          0          0          0          0  Message size for gather
      1000          4          4          4          0  Message size for reduce
         1      1E+06      1E+06      1E+06          0  Message size for scatter
---------------------------------------------------------------------------------------

FUNCTION SUMMARY (total):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.176     1:11.790           4           4   17947593 .TAU application
100.0           56     1:11.790           4       20040   17947549 addr=<0x4330e0>
 96.5     1:09.171     1:09.289        4000      800008      17322 addr=<0x433e70>
  1.6        1,163        1,163           4           0     290977 MPI_Init_thread()
  0.9          651          651        6000           0        109 MPI_Recv()
  0.5          329          329           4           0      82498 MPI_Finalize()
  0.4          253          253        4000           0         63 MPI_Reduce()
  0.1           59           59      400004           0          0 addr=<0x433ac0> [THROTTLED]
  0.1           58           58      400004           0          0 addr=<0x433a50> [THROTTLED]
  0.0           19           19           8           0       2422 addr=<0x433cf0>
  0.0           11           11        6000           0          2 MPI_Send()
  0.0            6            6           4           0       1711 MPI_Gather()
  0.0            6            6           4           0       1638 MPI_Scatter()
  0.0            1            1           4           0        443 addr=<0x433b20>
  0.0        0.215        0.215           4           0         54 MPI_Bcast()
  0.0        0.001        0.001           4           0          0 MPI_Comm_size()
  0.0            0            0           4           0          0 MPI_Comm_rank()

FUNCTION SUMMARY (mean):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.044       17,947           1           1   17947593 .TAU application
100.0           14       17,947           1        5010   17947549 addr=<0x4330e0>
 96.5       17,292       17,322        1000      200002      17322 addr=<0x433e70>
  1.6          290          290           1           0     290977 MPI_Init_thread()
  0.9          162          162        1500           0        109 MPI_Recv()
  0.5           82           82           1           0      82498 MPI_Finalize()
  0.4           63           63        1000           0         63 MPI_Reduce()
  0.1           14           14      100001           0          0 addr=<0x433ac0> [THROTTLED]
  0.1           14           14      100001           0          0 addr=<0x433a50> [THROTTLED]
  0.0            4            4           2           0       2422 addr=<0x433cf0>
  0.0            2            2        1500           0          2 MPI_Send()
  0.0            1            1           1           0       1711 MPI_Gather()
  0.0            1            1           1           0       1638 MPI_Scatter()
  0.0        0.443        0.443           1           0        443 addr=<0x433b20>
  0.0       0.0537       0.0537           1           0         54 MPI_Bcast()
  0.0      0.00025      0.00025           1           0          0 MPI_Comm_size()
  0.0            0            0           1           0          0 MPI_Comm_rank()



ppM-1-1@aolin21:~/Escritorio/MPI/Basic_TAU_FINAL/Hybrid/HYCore2$ paraprof
Exception in thread "AWT-EventQueue-0" java.awt.HeadlessException:
No X11 DISPLAY variable was set, but this program performed an operation which requires it.
        at java.awt.GraphicsEnvironment.checkHeadless(GraphicsEnvironment.java:204)
        at java.awt.Window.<init>(Window.java:536)
        at java.awt.Frame.<init>(Frame.java:420)
        at java.awt.Frame.<init>(Frame.java:385)
        at javax.swing.JFrame.<init>(JFrame.java:189)
        at edu.uoregon.tau.paraprof.ParaProfErrorDialog.<init>(ParaProfErrorDialog.java:37)
        at edu.uoregon.tau.paraprof.ParaProfUtils.handleException(ParaProfUtils.java:1719)
        at edu.uoregon.tau.paraprof.ParaProf$1.run(ParaProf.java:754)
        at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311)
        at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:756)
        at java.awt.EventQueue.access$500(EventQueue.java:97)
        at java.awt.EventQueue$3.run(EventQueue.java:709)
        at java.awt.EventQueue$3.run(EventQueue.java:703)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:80)
        at java.awt.EventQueue.dispatchEvent(EventQueue.java:726)
        at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
        at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
        at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
        at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
        at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
        at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)



